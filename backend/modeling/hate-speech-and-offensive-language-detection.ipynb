{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e369888d",
   "metadata": {
    "papermill": {
     "duration": 0.006026,
     "end_time": "2023-12-14T19:09:14.948726",
     "exception": false,
     "start_time": "2023-12-14T19:09:14.942700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Steps:\n",
    "1) preprocessing\n",
    "2) splitting\n",
    "3) tokenize & padding\n",
    "4) Create model & train\n",
    "5) evaluate\n",
    "\n",
    "# Preprocessing (cleaning the datasets):\n",
    "\n",
    "1) remove html entity\n",
    "2) change user tags (@xxx -> user)\n",
    "3) remove urls\n",
    "4) remove unnecessary symbol ('', !, \", ') -> cause a lot of noise in the dataset\n",
    "5) remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668587ef",
   "metadata": {
    "papermill": {
     "duration": 0.005948,
     "end_time": "2023-12-14T19:09:14.961120",
     "exception": false,
     "start_time": "2023-12-14T19:09:14.955172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1| Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77648b5-8e29-499a-b5ad-87d6633e16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114744d4-2336-4ec9-bd8b-73f9a8c04c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e10d7b1-6fcf-4102-abba-568980e1d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb30c43-2f4a-4c9f-bd55-d993f81c3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # to use word tokenize (split the sentence into words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fd2c21-6ea8-49b1-98af-05a1d7ab4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/djiemboutienctheuvictornico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b4063f-6a97-466c-9b47-97577c190a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/djiemboutienctheuvictornico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab75e763",
   "metadata": {
    "papermill": {
     "duration": 12.17501,
     "end_time": "2023-12-14T19:09:27.142267",
     "exception": false,
     "start_time": "2023-12-14T19:09:14.967257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd # read the csv\n",
    "import re # regex to detect username, url, html entity \n",
    "from nltk.corpus import stopwords # to remove the stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c8588",
   "metadata": {
    "papermill": {
     "duration": 0.006227,
     "end_time": "2023-12-14T19:09:27.155057",
     "exception": false,
     "start_time": "2023-12-14T19:09:27.148830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2| read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a5773f",
   "metadata": {
    "papermill": {
     "duration": 0.100274,
     "end_time": "2023-12-14T19:09:27.261585",
     "exception": false,
     "start_time": "2023-12-14T19:09:27.161311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>offensive_language_count</th>\n",
       "      <th>neither_count</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech_count  offensive_language_count  neither_count  class  \\\n",
       "0      3                  0                         0              3      2   \n",
       "1      3                  0                         3              0      1   \n",
       "2      3                  0                         3              0      1   \n",
       "3      3                  0                         2              1      1   \n",
       "4      6                  0                         6              0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ebc0f7",
   "metadata": {
    "papermill": {
     "duration": 0.020652,
     "end_time": "2023-12-14T19:09:27.289495",
     "exception": false,
     "start_time": "2023-12-14T19:09:27.268843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of tweets: (24783, 6)\n"
     ]
    }
   ],
   "source": [
    "# dataset shape to know how many tweets in the datasets\n",
    "print(f\"num of tweets: {data.shape}\")\n",
    "\n",
    "# extract the text and labels\n",
    "tweet = list(data['tweet'])\n",
    "labels = list(data['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470cd1cf",
   "metadata": {
    "papermill": {
     "duration": 0.00636,
     "end_time": "2023-12-14T19:09:27.302322",
     "exception": false,
     "start_time": "2023-12-14T19:09:27.295962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3| functions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f83112",
   "metadata": {
    "papermill": {
     "duration": 0.023981,
     "end_time": "2023-12-14T19:09:27.332726",
     "exception": false,
     "start_time": "2023-12-14T19:09:27.308745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#notes : all of the function taking 1 text at a time\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# add rt to remove retweet in dataset (noise)\n",
    "stop_words.add(\"rt\")\n",
    "\n",
    "# remove html entity:\n",
    "def remove_entity(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    text = re.sub(entity_regex, \"\", raw_text)\n",
    "    return text\n",
    "\n",
    "# change the user tags\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove urls\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove unnecessary symbols\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "## this function in to clean all the dataset by utilizing all the function above\n",
    "def preprocess(datas):\n",
    "    clean = []\n",
    "    # change the @xxx into \"user\"\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    # remove emojis (specifically unicode emojis)\n",
    "    clean = [remove_entity(text) for text in clean]\n",
    "    # remove urls\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    # remove trailing stuff\n",
    "    clean = [remove_noise_symbols(text) for text in clean]\n",
    "    # remove stopwords\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc62fa0d",
   "metadata": {
    "papermill": {
     "duration": 5.491728,
     "end_time": "2023-12-14T19:09:32.831006",
     "exception": false,
     "start_time": "2023-12-14T19:09:27.339278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call the cleaning function\n",
    "clean_tweet = preprocess(tweet)\n",
    "#clean_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14902565",
   "metadata": {
    "papermill": {
     "duration": 0.006479,
     "end_time": "2023-12-14T19:09:32.844323",
     "exception": false,
     "start_time": "2023-12-14T19:09:32.837844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 4| Splitting the dataset into test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "899c356d",
   "metadata": {
    "papermill": {
     "duration": 0.027113,
     "end_time": "2023-12-14T19:09:32.877998",
     "exception": false,
     "start_time": "2023-12-14T19:09:32.850885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweet, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ea6132",
   "metadata": {
    "papermill": {
     "duration": 0.709423,
     "end_time": "2023-12-14T19:09:33.593790",
     "exception": false,
     "start_time": "2023-12-14T19:09:32.884367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Tokenizing -> basically we use tokenisation for many things, its commonly used for feature extraction in preprocessing. btw idk how it works as feature extraction tho :(\n",
    "# declare the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# build the vocabulary based on train dataset\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenize the train and test dataset\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# vocabulary size (num of unique words) -> will be used in embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21dfb59a",
   "metadata": {
    "papermill": {
     "duration": 0.116562,
     "end_time": "2023-12-14T19:09:33.717276",
     "exception": false,
     "start_time": "2023-12-14T19:09:33.600714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9521b5cc",
   "metadata": {
    "papermill": {
     "duration": 0.015717,
     "end_time": "2023-12-14T19:09:33.739731",
     "exception": false,
     "start_time": "2023-12-14T19:09:33.724014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create hot_labels (idk whty tapi ini penting, kalo ga bakal error)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "y_train = to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "850a3b71",
   "metadata": {
    "papermill": {
     "duration": 0.014126,
     "end_time": "2023-12-14T19:09:33.760507",
     "exception": false,
     "start_time": "2023-12-14T19:09:33.746381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test tweet: 4957\n",
      "num train tweet: 19826\n"
     ]
    }
   ],
   "source": [
    "# another look on the number of tweet in test and training data\n",
    "\n",
    "print(f\"num test tweet: {y_test.shape[0]}\")\n",
    "print(f\"num train tweet: {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e448587",
   "metadata": {
    "papermill": {
     "duration": 0.006263,
     "end_time": "2023-12-14T19:09:33.773255",
     "exception": false,
     "start_time": "2023-12-14T19:09:33.766992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5| Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa113a4a",
   "metadata": {
    "papermill": {
     "duration": 0.016174,
     "end_time": "2023-12-14T19:09:33.795867",
     "exception": false,
     "start_time": "2023-12-14T19:09:33.779693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fbbc9bb",
   "metadata": {
    "papermill": {
     "duration": 3.024844,
     "end_time": "2023-12-14T19:09:36.827341",
     "exception": false,
     "start_time": "2023-12-14T19:09:33.802497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/djiemboutienctheuvictornico/Documents/MyFolders/helpProject/JUIN1305_1805/HateSpeechDetection/backend/.env/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# change dis if u want\n",
    "output_dim = 200\n",
    "\n",
    "# model = Sequential()\n",
    "# # embedding layer is like idk\n",
    "# model.add(Embedding(vocab_size, output_dim))\n",
    "# # lstm for xxx\n",
    "# model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "# # dropout to prevent overfitting\n",
    "# model.add(Dropout(0.5))\n",
    "# # dense to connect the previous output with current layer\n",
    "# model.add(Dense(128, activation=\"relu\"))\n",
    "# # dropout to prevent overfitting\n",
    "# model.add(Dropout(0.5))\n",
    "# # this is output layer, with 3 class (0, 1, 2)\n",
    "# model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# LSTM model architechture (CNN + LSTM)\n",
    "# model = Sequential([\n",
    "#     # embedding layer is like idk\n",
    "#     Embedding(vocab_size, output_dim),\n",
    "#     # lstm for xxx\n",
    "#     LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "#     # dropout to prevent overfitting\n",
    "#     Dropout(0.5),\n",
    "#     # dense to connect the previous output with current layer\n",
    "#     Dense(128, activation=\"relu\"),\n",
    "#     # dropout to prevent overfitting\n",
    "#     Dropout(0.5),\n",
    "#     # this is output layer, with 3 class (0, 1, 2)\n",
    "#     Dense(3, activation=\"softmax\"),\n",
    "# ])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, output_dim))\n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a053bf4",
   "metadata": {
    "papermill": {
     "duration": 0.037598,
     "end_time": "2023-12-14T19:09:36.871798",
     "exception": false,
     "start_time": "2023-12-14T19:09:36.834200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking the model parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c83e48",
   "metadata": {
    "papermill": {
     "duration": 385.45119,
     "end_time": "2023-12-14T19:16:02.331269",
     "exception": false,
     "start_time": "2023-12-14T19:09:36.880079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api.backend' has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MyFolders/helpProject/JUIN1305_1805/HateSpeechDetection/backend/.env/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mf1\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1\u001b[39m(y_true, y_pred):\n\u001b[0;32m---> 14\u001b[0m     precisions \u001b[38;5;241m=\u001b[39m \u001b[43mprecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     recalls \u001b[38;5;241m=\u001b[39m recall(y_true, y_pred)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m((precisions\u001b[38;5;241m*\u001b[39mrecalls)\u001b[38;5;241m/\u001b[39m(precisions\u001b[38;5;241m+\u001b[39mrecalls\u001b[38;5;241m+\u001b[39mK\u001b[38;5;241m.\u001b[39mepsilon()))\n",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36mprecision\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision\u001b[39m(y_true, y_pred):\n\u001b[0;32m----> 8\u001b[0m     true_positives \u001b[38;5;241m=\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m(K\u001b[38;5;241m.\u001b[39mround(K\u001b[38;5;241m.\u001b[39mclip(y_true \u001b[38;5;241m*\u001b[39m y_pred, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m      9\u001b[0m     predicted_positives \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msum(K\u001b[38;5;241m.\u001b[39mround(K\u001b[38;5;241m.\u001b[39mclip(y_pred, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     10\u001b[0m     precision \u001b[38;5;241m=\u001b[39m true_positives \u001b[38;5;241m/\u001b[39m (predicted_positives \u001b[38;5;241m+\u001b[39m K\u001b[38;5;241m.\u001b[39mepsilon())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api.backend' has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a16c9",
   "metadata": {
    "papermill": {
     "duration": 0.559682,
     "end_time": "2023-12-14T19:16:03.148779",
     "exception": false,
     "start_time": "2023-12-14T19:16:02.589097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "hist = model.history.history\n",
    "plt.plot(hist['loss'],'r',linewidth=2, label='Training loss')\n",
    "plt.plot(hist['val_loss'], 'g',linewidth=2, label='Validation loss')\n",
    "plt.title('Hate Speech and Offensive language Model')\n",
    "plt.xlabel('Epochs numbers')\n",
    "plt.ylabel('MSE numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8ce31",
   "metadata": {
    "papermill": {
     "duration": 0.289642,
     "end_time": "2023-12-14T19:16:03.703363",
     "exception": false,
     "start_time": "2023-12-14T19:16:03.413721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Thanks alot upvotes ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd96ce91-76d4-4931-92ce-0d1472c61d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\n",
      "anyio==4.2.0\n",
      "appnope==0.1.4\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==2.4.1\n",
      "astunparse==1.6.3\n",
      "async-lru==2.0.4\n",
      "attrs==23.2.0\n",
      "Babel==2.14.0\n",
      "beautifulsoup4==4.12.3\n",
      "bleach==6.1.0\n",
      "blinker==1.7.0\n",
      "certifi==2023.11.17\n",
      "cffi==1.16.0\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "comm==0.2.1\n",
      "contourpy==1.2.0\n",
      "cycler==0.12.1\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "dnspython==2.6.1\n",
      "et-xmlfile==1.1.0\n",
      "exceptiongroup==1.2.0\n",
      "executing==2.0.1\n",
      "fastjsonschema==2.19.1\n",
      "Flask==3.0.3\n",
      "Flask-Cors==4.0.0\n",
      "Flask-JWT==0.2.0\n",
      "Flask-JWT-Extended==4.6.0\n",
      "Flask-PyMongo==2.3.0\n",
      "flatbuffers==24.3.25\n",
      "fonttools==4.47.2\n",
      "fqdn==1.5.1\n",
      "gast==0.5.4\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.63.0\n",
      "gunicorn==21.2.0\n",
      "h5py==3.11.0\n",
      "idna==3.6\n",
      "imbalanced-learn==0.11.0\n",
      "imgkit==1.2.3\n",
      "importlib==1.0.4\n",
      "importlib-metadata==7.0.1\n",
      "importlib-resources==6.1.1\n",
      "ipykernel==6.29.0\n",
      "ipython==8.18.1\n",
      "ipywidgets==8.1.1\n",
      "isoduration==20.11.0\n",
      "itsdangerous==2.1.2\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.3\n",
      "joblib==1.3.2\n",
      "json5==0.9.14\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.21.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter==1.0.0\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.9.0\n",
      "jupyter-lsp==2.2.2\n",
      "jupyter_client==8.6.0\n",
      "jupyter_core==5.7.1\n",
      "jupyter_server==2.12.5\n",
      "jupyter_server_terminals==0.5.1\n",
      "jupyterlab==4.0.11\n",
      "jupyterlab-widgets==3.0.9\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.25.2\n",
      "keras==3.3.3\n",
      "kiwisolver==1.4.5\n",
      "libclang==18.1.1\n",
      "Markdown==3.6\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib==3.8.2\n",
      "matplotlib-inline==0.1.6\n",
      "mdurl==0.1.2\n",
      "mistune==3.0.2\n",
      "ml-dtypes==0.3.2\n",
      "namex==0.0.8\n",
      "nbclient==0.9.0\n",
      "nbconvert==7.14.2\n",
      "nbformat==5.9.2\n",
      "nest-asyncio==1.5.9\n",
      "networkx==3.2.1\n",
      "nltk==3.8.1\n",
      "notebook==7.0.6\n",
      "notebook_shim==0.2.3\n",
      "numpy==1.26.3\n",
      "openpyxl==3.1.2\n",
      "opt-einsum==3.3.0\n",
      "optree==0.11.0\n",
      "overrides==7.4.0\n",
      "packaging==23.2\n",
      "pandas==2.1.4\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.3\n",
      "passlib==1.7.4\n",
      "peewee==3.17.1\n",
      "pexpect==4.9.0\n",
      "pillow==10.2.0\n",
      "platformdirs==4.1.0\n",
      "plotly==5.18.0\n",
      "prometheus-client==0.19.0\n",
      "prompt-toolkit==3.0.43\n",
      "protobuf==4.25.3\n",
      "psutil==5.9.7\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pycparser==2.21\n",
      "Pygments==2.17.2\n",
      "PyJWT==2.8.0\n",
      "pymongo==4.6.3\n",
      "pyparsing==3.1.1\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==1.0.1\n",
      "python-json-logger==2.0.7\n",
      "pytz==2023.3.post1\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.2\n",
      "qtconsole==5.5.1\n",
      "QtPy==2.4.1\n",
      "referencing==0.32.1\n",
      "regex==2024.5.10\n",
      "requests==2.31.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.7.1\n",
      "rpds-py==0.17.1\n",
      "scikit-learn==1.4.0\n",
      "scipy==1.11.4\n",
      "seaborn==0.13.1\n",
      "Send2Trash==1.8.2\n",
      "setuptools==69.5.1\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.5\n",
      "stack-data==0.6.3\n",
      "tenacity==8.2.3\n",
      "tensorboard==2.16.2\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.16.1\n",
      "termcolor==2.4.0\n",
      "terminado==0.18.0\n",
      "threadpoolctl==3.2.0\n",
      "tinycss2==1.2.1\n",
      "tomli==2.0.1\n",
      "tornado==6.4\n",
      "tqdm==4.66.1\n",
      "traitlets==5.14.1\n",
      "types-python-dateutil==2.8.19.20240106\n",
      "typing_extensions==4.9.0\n",
      "tzdata==2023.4\n",
      "uri-template==1.3.0\n",
      "urllib3==2.1.0\n",
      "wcwidth==0.2.13\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.7.0\n",
      "Werkzeug==3.0.2\n",
      "wheel==0.43.0\n",
      "widgetsnbextension==4.0.9\n",
      "wrapt==1.16.0\n",
      "xgboost==2.0.3\n",
      "zipp==3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad933af3-8576-4f76-8766-76980e4c8d88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.utils.vis_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_to_dot\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVG\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlivelossplot\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.vis_utils'"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "import livelossplot\n",
    "\n",
    "plot_losses = livelossplot.PlotLossesKeras()\n",
    "%matplotlib inline\n",
    "\n",
    "NUM_ROWS = 28\n",
    "NUM_COLS = 28\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "def data_summary(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Summarize current state of dataset\"\"\"\n",
    "    print('Train images shape:', X_train.shape)\n",
    "    print('Train labels shape:', y_train.shape)\n",
    "    print('Test images shape:', X_test.shape)\n",
    "    print('Test labels shape:', y_test.shape)\n",
    "    print('Train labels:', y_train)\n",
    "    print('Test labels:', y_test)\n",
    "\n",
    "\"\"\"Load and prepare data\"\"\"\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# Check state of dataset\n",
    "data_summary(X_train, y_train, X_test, y_test)\n",
    "# Reshape data\n",
    "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
    "X_test = X_test.astype('float32') / 255\n",
    "# Categorically encode labels\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "# Check state of dataset\n",
    "data_summary(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\"\"\"Build and train neural network\"\"\"\n",
    "# Build neural network\n",
    "model = models.Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train,\n",
    "batch_size=BATCH_SIZE,\n",
    "epochs=EPOCHS,\n",
    "callbacks=[plot_losses],\n",
    "verbose=1,\n",
    "validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\"\"\"Summarize and visualize the model\"\"\"\n",
    "# Summary of neural network\n",
    "model.summary()\n",
    "# Output network visualization\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4599f3-91d2-4f82-9a11-ea6ab1974866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4096079,
     "sourceId": 7105074,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 415.657241,
   "end_time": "2023-12-14T19:16:06.580704",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-14T19:09:10.923463",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
