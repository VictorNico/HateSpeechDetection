{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7105074,
     "sourceType": "datasetVersion",
     "datasetId": 4096079
    }
   ],
   "dockerImageVersionId": 30627,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Steps:\n1) preprocessing\n\n2) splitting\n\n3) tokenize & padding\n\n4) Create model & train\n\n5) evaluate\n\n# Preprocessing (cleaning the datasets):\n\n1) remove html entity\n\n2) change user tags (@xxx -> user)\n\n3) remove urls\n\n4) remove unnecessary symbol ('', !, \", ') -> cause a lot of noise in the dataset\n\n5) remove stopwords",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# 1| Import libraries",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "1. __pandas__: This module is used for data manipulation and analysis. It provides flexible data structures, including the DataFrame, which allows you to read, write, and perform operations on tabular data such as CSV files in your case.\n\n2. __re (regex)__: This module is used for working with regular expressions in Python. Regular expressions are sequences of characters used for searching and manipulating text strings. In your code, it is used to detect specific patterns such as usernames, URLs, and HTML entities in the text.\n\n3. __nltk (Natural Language Toolkit)__: It is a popular Python library for natural language processing. It provides various functionalities and linguistic resources for working with text data. In your code, it is used to perform word tokenization, which means splitting a sentence into individual words or tokens.\n\n4. __nltk.corpus.stopwords__: This is a sub-module of NLTK that contains a list of commonly used words called \"stop words.\" These words are typically empty or uninformative words such as \"the,\" \"and,\" \"but,\" etc. They are often removed during text processing to reduce noise and improve performance.\n\n5. __sklearn.model_selection.train_test_split__: It is a function from scikit-learn that allows you to split data into training and test sets. This is often used to evaluate the performance of a model by reserving a portion of the data for testing.\n\n7. __tensorflow.keras.preprocessing.text.Tokenizer__: This is a class from TensorFlow-Keras used to convert text into sequences of integers. It is useful for text vectorization before feeding it to a machine learning model.\n\n6. __tensorflow.keras.preprocessing.sequence.pad_sequences__: This function from TensorFlow-Keras is used to pad sequences of integers to a uniform length. It is often used to prepare text data before using it in neural network models.\n\n8. __tensorflow.keras.models.Sequential__: It is a class from TensorFlow-Keras that allows you to create sequential neural network models, where layers are stacked on top of each other in a specific order.\n\n9. __tensorflow.keras.layers.Embedding__: This layer in TensorFlow-Keras is used for word representation as dense vectors in a reduced-dimensional space. It is often used as the first layer in natural language processing models.\n\n10. __tensorflow.keras.layers.LSTM__: It is a recurrent neural network (RNN) layer called Long Short-Term Memory (LSTM). It is used for sequence processing and is particularly suitable for text analysis.\n\n11. __tensorflow.keras.layers.Dense__: This layer in TensorFlow-Keras is a fully connected dense layer, where each neuron is connected to all neurons in the previous layer. It is used to represent fully connected layers in a neural network.\n\n12. __tensorflow.keras.layers.Dropout__: This layer in TensorFlow-Keras is used to apply a regularization technique called \"dropout.\" It randomly deactivates some neurons during training, which helps prevent overfitting of the model.\n\n13. __keras.utils.to_categorical__: This function from Keras is used to convert categorical variables into binary variables. It is often used to convert labels or classes into one-hot encoding.\n\n14. __keras.backend__: It is a Keras module that provides low-level functions and operations to interact with the underlying backend, such as TensorFlow in this case.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd # read the csv\nimport re # regex to detect username, url, html entity \nimport nltk # to use word tokenize (split the sentence into words)\nfrom nltk.corpus import stopwords # to remove the stopwords\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n\nfrom keras.utils import to_categorical\nfrom keras import backend as K",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:08.151002Z",
     "iopub.execute_input": "2024-05-15T09:45:08.152195Z",
     "iopub.status.idle": "2024-05-15T09:45:08.289752Z",
     "shell.execute_reply.started": "2024-05-15T09:45:08.152160Z",
     "shell.execute_reply": "2024-05-15T09:45:08.288611Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:23:42.576273Z",
     "start_time": "2024-05-15T12:23:29.842903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 13:23:37.714624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "# 2| read the data\n\n\nThis dataset, named hate_speech_offensive, is a meticulously curated collection of annotated tweets with the specific purpose of detecting hate speech and offensive language. The dataset primarily consists of English tweets and is designed to train machine learning models or algorithms in the task of hate speech detection. It should be noted that the dataset has not been divided into multiple subsets, and only the train split is currently available for use.\n\nThe dataset includes several columns that provide valuable information for understanding each tweet's classification. The column \n- __count__ represents the total number of annotations provided for each tweet, \n\n- whereas __hate_speech_count__ signifies how many annotations classified a particular tweet as hate speech. \n\n- On the other hand, __offensive_language_count__ indicates the number of annotations categorizing a tweet as containing offensive language. \n\n- Additionally, __neither_count__ denotes how many annotations identified a tweet as neither hate speech nor offensive language.\n\nFor researchers and developers aiming to create effective models or algorithms capable of detecting hate speech and offensive language on Twitter, this comprehensive dataset offers a rich resource for training and evaluation purposes. [Read more](https://www.kaggle.com/datasets/thedevastator/hate-speech-and-offensive-language-detection)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# show content of stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T10:22:48.463310Z",
     "iopub.execute_input": "2024-05-15T10:22:48.463677Z",
     "iopub.status.idle": "2024-05-15T10:22:48.474343Z",
     "shell.execute_reply.started": "2024-05-15T10:22:48.463645Z",
     "shell.execute_reply": "2024-05-15T10:22:48.473221Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:22:52.331011Z",
     "start_time": "2024-05-15T12:22:52.297746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/djiemboutienctheuvictornico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/djiemboutienctheuvictornico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpunkt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstopwords\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mstop_words\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "data.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:08.291684Z",
     "iopub.execute_input": "2024-05-15T09:45:08.292009Z",
     "iopub.status.idle": "2024-05-15T09:45:08.397765Z",
     "shell.execute_reply.started": "2024-05-15T09:45:08.291982Z",
     "shell.execute_reply": "2024-05-15T09:45:08.396653Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:25:37.077444Z",
     "start_time": "2024-05-15T12:25:37.007693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   count  hate_speech_count  offensive_language_count  neither_count  class  \\\n",
       "0      3                  0                         0              3      2   \n",
       "1      3                  0                         3              0      1   \n",
       "2      3                  0                         3              0      1   \n",
       "3      3                  0                         2              1      1   \n",
       "4      6                  0                         6              0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>offensive_language_count</th>\n",
       "      <th>neither_count</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "# show the values of class\ndata['class'].unique()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T10:06:52.158021Z",
     "iopub.execute_input": "2024-05-15T10:06:52.158853Z",
     "iopub.status.idle": "2024-05-15T10:06:52.168861Z",
     "shell.execute_reply.started": "2024-05-15T10:06:52.158816Z",
     "shell.execute_reply": "2024-05-15T10:06:52.167789Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:11.085594Z",
     "start_time": "2024-05-15T12:26:11.078355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "# hate speech class\ndata[data['class'] == 0]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T10:08:56.960793Z",
     "iopub.execute_input": "2024-05-15T10:08:56.961166Z",
     "iopub.status.idle": "2024-05-15T10:08:56.981580Z",
     "shell.execute_reply.started": "2024-05-15T10:08:56.961137Z",
     "shell.execute_reply": "2024-05-15T10:08:56.980478Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:11.928750Z",
     "start_time": "2024-05-15T12:26:11.911325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       count  hate_speech_count  offensive_language_count  neither_count  \\\n",
       "85         3                  2                         1              0   \n",
       "89         3                  3                         0              0   \n",
       "110        3                  3                         0              0   \n",
       "184        3                  3                         0              0   \n",
       "202        3                  2                         1              0   \n",
       "...      ...                ...                       ...            ...   \n",
       "24576      3                  2                         1              0   \n",
       "24685      3                  2                         1              0   \n",
       "24751      3                  2                         1              0   \n",
       "24776      3                  3                         0              0   \n",
       "24777      3                  2                         1              0   \n",
       "\n",
       "       class                                              tweet  \n",
       "85         0  \"@Blackman38Tide: @WhaleLookyHere @HowdyDowdy1...  \n",
       "89         0  \"@CB_Baby24: @white_thunduh alsarabsss\" hes a ...  \n",
       "110        0  \"@DevilGrimz: @VigxRArts you're fucking gay, b...  \n",
       "184        0  \"@MarkRoundtreeJr: LMFAOOOO I HATE BLACK PEOPL...  \n",
       "202        0  \"@NoChillPaz: \"At least I'm not a nigger\" http...  \n",
       "...      ...                                                ...  \n",
       "24576      0                this guy is the biggest faggot omfg  \n",
       "24685      0  which one of these names is more offensive kik...  \n",
       "24751      0         you a pussy ass nigga and I know it nigga.  \n",
       "24776      0                                 you're all niggers  \n",
       "24777      0  you're such a retard i hope you get type 2 dia...  \n",
       "\n",
       "[1430 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>offensive_language_count</th>\n",
       "      <th>neither_count</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@Blackman38Tide: @WhaleLookyHere @HowdyDowdy1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@CB_Baby24: @white_thunduh alsarabsss\" hes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@DevilGrimz: @VigxRArts you're fucking gay, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@MarkRoundtreeJr: LMFAOOOO I HATE BLACK PEOPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@NoChillPaz: \"At least I'm not a nigger\" http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24576</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this guy is the biggest faggot omfg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24685</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>which one of these names is more offensive kik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24751</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you a pussy ass nigga and I know it nigga.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24776</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you're all niggers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24777</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you're such a retard i hope you get type 2 dia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1430 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# offensive language class\ndata[data['class'] == 1]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T10:09:21.033953Z",
     "iopub.execute_input": "2024-05-15T10:09:21.034860Z",
     "iopub.status.idle": "2024-05-15T10:09:21.051516Z",
     "shell.execute_reply.started": "2024-05-15T10:09:21.034820Z",
     "shell.execute_reply": "2024-05-15T10:09:21.050444Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:12.913672Z",
     "start_time": "2024-05-15T12:26:12.899200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       count  hate_speech_count  offensive_language_count  neither_count  \\\n",
       "1          3                  0                         3              0   \n",
       "2          3                  0                         3              0   \n",
       "3          3                  0                         2              1   \n",
       "4          6                  0                         6              0   \n",
       "5          3                  1                         2              0   \n",
       "...      ...                ...                       ...            ...   \n",
       "24774      3                  0                         3              0   \n",
       "24775      3                  0                         3              0   \n",
       "24778      3                  0                         2              1   \n",
       "24780      3                  0                         3              0   \n",
       "24781      6                  0                         6              0   \n",
       "\n",
       "       class                                              tweet  \n",
       "1          1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2          1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3          1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4          1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "5          1  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...  \n",
       "...      ...                                                ...  \n",
       "24774      1  you really care bout dis bitch. my dick all in...  \n",
       "24775      1   you worried bout other bitches, you need me for?  \n",
       "24778      1  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
       "24780      1  young buck wanna eat!!.. dat nigguh like I ain...  \n",
       "24781      1              youu got wild bitches tellin you lies  \n",
       "\n",
       "[19190 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>offensive_language_count</th>\n",
       "      <th>neither_count</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24774</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you really care bout dis bitch. my dick all in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24775</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you worried bout other bitches, you need me for?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19190 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "# normal language class\ndata[data['class'] == 2]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T10:09:30.946408Z",
     "iopub.execute_input": "2024-05-15T10:09:30.947348Z",
     "iopub.status.idle": "2024-05-15T10:09:30.964596Z",
     "shell.execute_reply.started": "2024-05-15T10:09:30.947307Z",
     "shell.execute_reply": "2024-05-15T10:09:30.963436Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:13.902634Z",
     "start_time": "2024-05-15T12:26:13.890591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       count  hate_speech_count  offensive_language_count  neither_count  \\\n",
       "0          3                  0                         0              3   \n",
       "40         3                  0                         1              2   \n",
       "63         3                  0                         0              3   \n",
       "66         3                  0                         1              2   \n",
       "67         3                  0                         1              2   \n",
       "...      ...                ...                       ...            ...   \n",
       "24736      3                  0                         0              3   \n",
       "24737      3                  0                         1              2   \n",
       "24767      3                  0                         1              2   \n",
       "24779      3                  0                         1              2   \n",
       "24782      3                  0                         0              3   \n",
       "\n",
       "       class                                              tweet  \n",
       "0          2  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "40         2    \" momma said no pussy cats inside my doghouse \"  \n",
       "63         2  \"@Addicted2Guys: -SimplyAddictedToGuys http://...  \n",
       "66         2  \"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...  \n",
       "67         2  \"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these...  \n",
       "...      ...                                                ...  \n",
       "24736      2  yaya ho.. cute avi tho RT @ViVaLa_Ari I had no...  \n",
       "24737      2  yea so about @N_tel 's new friend.. all my fri...  \n",
       "24767      2  you know what they say, the early bird gets th...  \n",
       "24779      2  you've gone and broke the wrong heart baby, an...  \n",
       "24782      2  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
       "\n",
       "[4163 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>offensive_language_count</th>\n",
       "      <th>neither_count</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\" momma said no pussy cats inside my doghouse \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@Addicted2Guys: -SimplyAddictedToGuys http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"@Allyhaaaaa: Lemmie eat a Oreo &amp;amp; do these...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24736</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>yaya ho.. cute avi tho RT @ViVaLa_Ari I had no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24737</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yea so about @N_tel 's new friend.. all my fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24767</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you know what they say, the early bird gets th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4163 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "# dataset shape to know how many tweets in the datasets\nprint(f\"num of tweets: {data.shape}\")\n\n# extract the text and labels\ntweet = list(data['tweet'])\nlabels = list(data['class'])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:08.398984Z",
     "iopub.execute_input": "2024-05-15T09:45:08.399355Z",
     "iopub.status.idle": "2024-05-15T09:45:08.413516Z",
     "shell.execute_reply.started": "2024-05-15T09:45:08.399315Z",
     "shell.execute_reply": "2024-05-15T09:45:08.412355Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:16.210349Z",
     "start_time": "2024-05-15T12:26:16.195155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of tweets: (24783, 6)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "# 3| functions to clean the data",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:40.991937Z",
     "start_time": "2024-05-15T12:26:40.981534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#notes : all of the function taking 1 text at a time\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# add rt to remove retweet in dataset (noise)\n",
    "stop_words.add(\"rt\")\n",
    "stop_words"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 'rt',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# remove html entity:\n",
    "def remove_entity(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    text = re.sub(entity_regex, \"\", raw_text)\n",
    "    return text\n",
    "\n",
    "# change the user tags\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove urls\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove unnecessary symbols\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "## this function in to clean all the dataset by utilizing all the function above\n",
    "def preprocess(datas):\n",
    "    clean = []\n",
    "    # change the @xxx into \"user\"\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    # remove emojis (specifically unicode emojis)\n",
    "    clean = [remove_entity(text) for text in clean]\n",
    "    # remove urls\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    # remove trailing stuff\n",
    "    clean = [remove_noise_symbols(text) for text in clean]\n",
    "    # remove stopwords\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "\n",
    "    return clean"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:08.416497Z",
     "iopub.execute_input": "2024-05-15T09:45:08.417280Z",
     "iopub.status.idle": "2024-05-15T09:45:08.435797Z",
     "shell.execute_reply.started": "2024-05-15T09:45:08.417242Z",
     "shell.execute_reply": "2024-05-15T09:45:08.434803Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:50.046240Z",
     "start_time": "2024-05-15T12:26:50.037007Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "# call the cleaning function\nclean_tweet = preprocess(tweet)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:08.437654Z",
     "iopub.execute_input": "2024-05-15T09:45:08.438010Z",
     "iopub.status.idle": "2024-05-15T09:45:15.159331Z",
     "shell.execute_reply.started": "2024-05-15T09:45:08.437975Z",
     "shell.execute_reply": "2024-05-15T09:45:15.158534Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:56.145836Z",
     "start_time": "2024-05-15T12:26:51.357812Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "clean_tweet[:5]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:15.160972Z",
     "iopub.execute_input": "2024-05-15T09:45:15.161279Z",
     "iopub.status.idle": "2024-05-15T09:45:15.167174Z",
     "shell.execute_reply.started": "2024-05-15T09:45:15.161252Z",
     "shell.execute_reply": "2024-05-15T09:45:15.166279Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:26:56.154341Z",
     "start_time": "2024-05-15T12:26:56.148929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user woman shouldnt complain cleaning house . man always take trash .',\n",
       " 'user boy dats cold.tyga dwn bad cuffin dat hoe 1st place',\n",
       " 'user Dawg user ever fuck bitch start cry ? confused shit',\n",
       " 'user user look like tranny',\n",
       " 'user shit hear might true might faker bitch told ya']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "# 4| Splitting the dataset into test and validation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(clean_tweet, labels, test_size=0.2, random_state=42)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:15.168617Z",
     "iopub.execute_input": "2024-05-15T09:45:15.168992Z",
     "iopub.status.idle": "2024-05-15T09:45:15.193683Z",
     "shell.execute_reply.started": "2024-05-15T09:45:15.168956Z",
     "shell.execute_reply": "2024-05-15T09:45:15.192824Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:27:01.918237Z",
     "start_time": "2024-05-15T12:27:01.903098Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "## Tokenizing -> basically we use tokenisation for many things, its commonly used for feature extraction in preprocessing. btw idk how it works as feature extraction tho :(\n# declare the tokenizer\ntokenizer = Tokenizer()\n# build the vocabulary based on train dataset\ntokenizer.fit_on_texts(X_train)\n# tokenize the train and test dataset\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\n# vocabulary size (num of unique words) -> will be used in embedding layer\nvocab_size = len(tokenizer.word_index) + 1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:15.194958Z",
     "iopub.execute_input": "2024-05-15T09:45:15.195308Z",
     "iopub.status.idle": "2024-05-15T09:45:15.979401Z",
     "shell.execute_reply.started": "2024-05-15T09:45:15.195279Z",
     "shell.execute_reply": "2024-05-15T09:45:15.978260Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:27:04.217384Z",
     "start_time": "2024-05-15T12:27:03.422833Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "# save the tokenizer fn\n",
    "joblib.dump(tokenizer,'../models/tokenizer.sav')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:15.980869Z",
     "iopub.execute_input": "2024-05-15T09:45:15.981231Z",
     "iopub.status.idle": "2024-05-15T09:45:16.831433Z",
     "shell.execute_reply.started": "2024-05-15T09:45:15.981177Z",
     "shell.execute_reply": "2024-05-15T09:45:16.830334Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:27:17.552979Z",
     "start_time": "2024-05-15T12:27:16.916467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/tokenizer.sav']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "joblib.dump(max_length,'../models/max_length.sav')\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "# uniformize sequences\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:16.834795Z",
     "iopub.execute_input": "2024-05-15T09:45:16.835112Z",
     "iopub.status.idle": "2024-05-15T09:45:16.967258Z",
     "shell.execute_reply.started": "2024-05-15T09:45:16.835085Z",
     "shell.execute_reply": "2024-05-15T09:45:16.966184Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:28:03.596617Z",
     "start_time": "2024-05-15T12:28:03.511602Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "# create hot_labels\ny_test = to_categorical(y_test, num_classes=3)\ny_train = to_categorical(y_train, num_classes=3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:16.968796Z",
     "iopub.execute_input": "2024-05-15T09:45:16.969124Z",
     "iopub.status.idle": "2024-05-15T09:45:16.976743Z",
     "shell.execute_reply.started": "2024-05-15T09:45:16.969095Z",
     "shell.execute_reply": "2024-05-15T09:45:16.975672Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:28:06.068104Z",
     "start_time": "2024-05-15T12:28:06.061218Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "(y_test,y_train)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:16.978293Z",
     "iopub.execute_input": "2024-05-15T09:45:16.978803Z",
     "iopub.status.idle": "2024-05-15T09:45:16.989407Z",
     "shell.execute_reply.started": "2024-05-15T09:45:16.978743Z",
     "shell.execute_reply": "2024-05-15T09:45:16.988166Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:28:07.149635Z",
     "start_time": "2024-05-15T12:28:07.143128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "# another look on the number of tweet in test and training data\n\nprint(f\"num test tweet: {y_test.shape[0]}\")\nprint(f\"num train tweet: {y_train.shape[0]}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:16.991263Z",
     "iopub.execute_input": "2024-05-15T09:45:16.991914Z",
     "iopub.status.idle": "2024-05-15T09:45:17.000400Z",
     "shell.execute_reply.started": "2024-05-15T09:45:16.991870Z",
     "shell.execute_reply": "2024-05-15T09:45:16.999198Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:28:08.237896Z",
     "start_time": "2024-05-15T12:28:08.234191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test tweet: 4957\n",
      "num train tweet: 19826\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": "# 5| Building the model",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "1. recall(y_true, y_pred):\n    - K.round(K.clip(y_true * y_pred, 0, 1)): This operation element-wise multiplies the true labels (y_true) and the model predictions (y_pred), and then rounds the results to the nearest integer. The K.clip() function is used to limit the resulting values between 0 and 1, keeping only the binary values. This yields a matrix of the same shape as y_true and y_pred, with 1s for true positives and 0s for other cases.\n    \n    - K.sum(K.round(K.clip(y_true * y_pred, 0, 1))): This operation calculates the sum of all elements in the matrix obtained in the previous step, giving the total number of true positives.\n\n    - K.sum(K.round(K.clip(y_true, 0, 1))): This operation calculates the sum of all elements in the y_true matrix, after rounding and clipping between 0 and 1. This corresponds to the total number of actual positives.\n\n    - recall = true_positives / (possible_positives + K.epsilon()): This operation calculates recall by dividing the number of true positives by the total number of actual positives. The value K.epsilon() is added to avoid division by zero.\n\n2. precision(y_true, y_pred):\n    - K.round(K.clip(y_true * y_pred, 0, 1)): This operation is similar to the one in recall(). It element-wise multiplies the true labels (y_true) and the model predictions (y_pred), and then rounds the results to the nearest integer. The K.clip() function is used to limit the resulting values between 0 and 1.\n    \n    - K.sum(K.round(K.clip(y_true * y_pred, 0, 1))): This operation calculates the total number of true positives.\n    \n    - K.sum(K.round(K.clip(y_pred, 0, 1))): This operation calculates the sum of all elements in the y_pred matrix, after rounding and clipping between 0 and 1. This corresponds to the total number of samples predicted as positives by the model.\n    \n    - precision = true_positives / (predicted_positives + K.epsilon()): This operation calculates precision by dividing the number of true positives by the total number of samples predicted as positives.\n\n3. f1(y_true, y_pred):\n    - precision = precision(y_true, y_pred): This operation calls the precision() function to calculate precision.\n    \n    - recall = recall(y_true, y_pred): This operation calls the recall() function to calculate recall.\n    \n    - f1 = 2 * ((precisions * recalls) / (precisions + recalls + K.epsilon())): This operation uses the precision (precisions) and recall (recalls) values to calculate the F1 score. The value K.epsilon() is added to avoid division by zero. The F1 score is a combined measure of precision and recall, providing an overall indication of the performance of a binary classification model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1(y_true, y_pred):\n    precisions = precision(y_true, y_pred)\n    recalls = recall(y_true, y_pred)\n    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:17.002008Z",
     "iopub.execute_input": "2024-05-15T09:45:17.002708Z",
     "iopub.status.idle": "2024-05-15T09:45:17.013380Z",
     "shell.execute_reply.started": "2024-05-15T09:45:17.002659Z",
     "shell.execute_reply": "2024-05-15T09:45:17.012270Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:28:13.668702Z",
     "start_time": "2024-05-15T12:28:13.662603Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": "# change dis if u want\noutput_dim = 200\n\n# LSTM model architechture (CNN + LSTM)\nmodel = Sequential([\n    # embedding layer is like idk\n    Embedding(vocab_size, output_dim, input_length=max_length),\n    # lstm for xxx\n    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n    # dropout to prevent overfitting\n    Dropout(0.5),\n    # dense to connect the previous output with current layer\n    Dense(128, activation=\"relu\"),\n    # dropout to prevent overfitting\n    Dropout(0.5),\n    # this is output layer, with 3 class (0, 1, 2)\n    Dense(3, activation=\"softmax\"),\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:17.015182Z",
     "iopub.execute_input": "2024-05-15T09:45:17.016321Z",
     "iopub.status.idle": "2024-05-15T09:45:17.239839Z",
     "shell.execute_reply.started": "2024-05-15T09:45:17.016282Z",
     "shell.execute_reply": "2024-05-15T09:45:17.238645Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:28:19.889377Z",
     "start_time": "2024-05-15T12:28:19.528471Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 13:28:19.552561: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "# checking the model parameters\nmodel.summary()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:17.241342Z",
     "iopub.execute_input": "2024-05-15T09:45:17.242732Z",
     "iopub.status.idle": "2024-05-15T09:45:17.272067Z",
     "shell.execute_reply.started": "2024-05-15T09:45:17.242695Z",
     "shell.execute_reply": "2024-05-15T09:45:17.271069Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:28:30.443207Z",
     "start_time": "2024-05-15T12:28:30.426935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 26, 200)           3734200   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                67840     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,810,747\n",
      "Trainable params: 3,810,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": "# Train the model\nmodel_history = model.fit(\n    X_train,\n    y_train,\n    batch_size = 64,\n    epochs=10,\n    validation_data=(X_test, y_test)\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:45:17.273409Z",
     "iopub.execute_input": "2024-05-15T09:45:17.273707Z",
     "iopub.status.idle": "2024-05-15T09:51:41.095262Z",
     "shell.execute_reply.started": "2024-05-15T09:45:17.273682Z",
     "shell.execute_reply": "2024-05-15T09:51:41.094372Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:34:08.736792Z",
     "start_time": "2024-05-15T12:28:36.590551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "310/310 [==============================] - 35s 103ms/step - loss: 0.4671 - accuracy: 0.8370 - f1: 0.8148 - precision: 0.8462 - recall: 0.7907 - val_loss: 0.3268 - val_accuracy: 0.8886 - val_f1: 0.8896 - val_precision: 0.8992 - val_recall: 0.8805\n",
      "Epoch 2/10\n",
      "310/310 [==============================] - 48s 154ms/step - loss: 0.2372 - accuracy: 0.9193 - f1: 0.9194 - precision: 0.9304 - recall: 0.9088 - val_loss: 0.3340 - val_accuracy: 0.8848 - val_f1: 0.8850 - val_precision: 0.8942 - val_recall: 0.8763\n",
      "Epoch 3/10\n",
      "310/310 [==============================] - 33s 107ms/step - loss: 0.1562 - accuracy: 0.9444 - f1: 0.9443 - precision: 0.9481 - recall: 0.9406 - val_loss: 0.3785 - val_accuracy: 0.8870 - val_f1: 0.8865 - val_precision: 0.8904 - val_recall: 0.8827\n",
      "Epoch 4/10\n",
      "310/310 [==============================] - 29s 95ms/step - loss: 0.1039 - accuracy: 0.9647 - f1: 0.9649 - precision: 0.9663 - recall: 0.9635 - val_loss: 0.4868 - val_accuracy: 0.8735 - val_f1: 0.8733 - val_precision: 0.8750 - val_recall: 0.8717\n",
      "Epoch 5/10\n",
      "310/310 [==============================] - 30s 96ms/step - loss: 0.0799 - accuracy: 0.9708 - f1: 0.9708 - precision: 0.9715 - recall: 0.9701 - val_loss: 0.5357 - val_accuracy: 0.8755 - val_f1: 0.8760 - val_precision: 0.8773 - val_recall: 0.8747\n",
      "Epoch 6/10\n",
      "310/310 [==============================] - 32s 105ms/step - loss: 0.0618 - accuracy: 0.9773 - f1: 0.9773 - precision: 0.9779 - recall: 0.9766 - val_loss: 0.6898 - val_accuracy: 0.8693 - val_f1: 0.8695 - val_precision: 0.8710 - val_recall: 0.8680\n",
      "Epoch 7/10\n",
      "310/310 [==============================] - 31s 100ms/step - loss: 0.0528 - accuracy: 0.9803 - f1: 0.9804 - precision: 0.9810 - recall: 0.9798 - val_loss: 0.6755 - val_accuracy: 0.8691 - val_f1: 0.8695 - val_precision: 0.8707 - val_recall: 0.8684\n",
      "Epoch 8/10\n",
      "310/310 [==============================] - 35s 112ms/step - loss: 0.0474 - accuracy: 0.9826 - f1: 0.9826 - precision: 0.9830 - recall: 0.9821 - val_loss: 0.7572 - val_accuracy: 0.8642 - val_f1: 0.8647 - val_precision: 0.8659 - val_recall: 0.8636\n",
      "Epoch 9/10\n",
      "310/310 [==============================] - 30s 98ms/step - loss: 0.0412 - accuracy: 0.9843 - f1: 0.9842 - precision: 0.9846 - recall: 0.9839 - val_loss: 0.7829 - val_accuracy: 0.8671 - val_f1: 0.8670 - val_precision: 0.8681 - val_recall: 0.8660\n",
      "Epoch 10/10\n",
      "310/310 [==============================] - 28s 91ms/step - loss: 0.0388 - accuracy: 0.9857 - f1: 0.9858 - precision: 0.9861 - recall: 0.9854 - val_loss: 0.8139 - val_accuracy: 0.8630 - val_f1: 0.8628 - val_precision: 0.8637 - val_recall: 0.8620\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "# save the tokenizer fn\n",
    "joblib.dump(model,'../models/model.sav')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:51:41.096614Z",
     "iopub.execute_input": "2024-05-15T09:51:41.096915Z",
     "iopub.status.idle": "2024-05-15T09:51:41.455378Z",
     "shell.execute_reply.started": "2024-05-15T09:51:41.096889Z",
     "shell.execute_reply": "2024-05-15T09:51:41.454387Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:34:13.790849Z",
     "start_time": "2024-05-15T12:34:08.739682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://52631a78-36f3-45a0-a427-f3b60966247f/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/model.sav']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nhist = model.history.history\nplt.plot(hist['loss'],'r',linewidth=2, label='Training loss')\nplt.plot(hist['val_loss'], 'g',linewidth=2, label='Validation loss')\nplt.title('Hate Speech and Offensive language Model')\nplt.xlabel('Epochs numbers')\nplt.ylabel('MSE numbers')\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-15T09:51:41.456624Z",
     "iopub.execute_input": "2024-05-15T09:51:41.456939Z",
     "iopub.status.idle": "2024-05-15T09:51:41.790182Z",
     "shell.execute_reply.started": "2024-05-15T09:51:41.456913Z",
     "shell.execute_reply": "2024-05-15T09:51:41.789189Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:34:13.831688Z",
     "start_time": "2024-05-15T12:34:13.793097Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      2\u001B[0m hist \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mhistory\u001B[38;5;241m.\u001B[39mhistory\n\u001B[1;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(hist[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m],\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m,linewidth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining loss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
